{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Fine-Tuning Example:\n",
      "Input Part:\n",
      " #### Customer Profile Summary\n",
      "- Name: Alex Trousers (K)\n",
      "- Age: 50\n",
      "- Membership: ACTIVE\n",
      "- Purchase History: Alex has purchased the following items:\n",
      "1. Alex Trousers (K) - Pyjama bottoms\n",
      "2. Anthony 3pk basic short trunks - Underwear bottoms\n",
      "3. Erik Tanktop 2 PK - Vest tops\n",
      "4. Roy Shorts - Shorts\n",
      "5. Summer price tee - T-shirts\n",
      "6. Bellini price - Sweaters\n",
      "7. Velvet twist tee - Tops\n",
      "8. Ringo hipbelt - Belts\n",
      "9. EMELIE DOUBLE EYELET BELT - Belts\n",
      "10. Perrie Slim HW Denim Shorts - Shorts\n",
      "11. Perrie Slim HW Denim Shorts - Shorts\n",
      "12. ES Dragonfly dress - Dresses\n",
      "13. Boy Denim Shorts - Shorts\n",
      "14. Skinny 5pkt Midprice - Trousers\n",
      "15. REX SLIM LS T-SHIRT - T-shirts\n",
      "16. Max 3pk Checks and Stripes - Trousers\n",
      "17. PETER POLO - Polo shirts\n",
      "18. Shaping Skinny HW - Trousers\n",
      "19. HAVANA HW tights - Tights\n",
      "20. SORRENTO RW trs - Trousers\n",
      "21. Tinos mesh tights - Leggings/Tights\n",
      "22. HEAVEN shaping HW tight - Leggings/Tights\n",
      "\n",
      "Output Part:\n",
      " 1. Cat Tee - A versatile t-shirt option for everyday wear.\n",
      "2. Cat Tee - A classic white tee for those who prefer a simple design.\n",
      "3. Hayes slim trouser - A stylish and comfortable suit option for work or leisure.\n",
      "4. PETER POLO - A high-quality polo shirt for business or casual occasions.\n",
      "5. Shaping Skinny HW - A flattering and comfortable pair of jeans for everyday wear.\n",
      "6. HAVANA HW tights - A pair of high-quality tights for colder weather.\n",
      "7. SORRENTO RW trs - A stylish and comfortable sports option for outdoor activities.\n",
      "8. Tinos mesh tights - A lightweight and breathable option for outdoor activities.\n",
      "9. HEAVEN shaping HW tight - A high-quality and comfortable legging option for lounging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 44.73 examples/s]\n",
      "C:\\Users\\bunny\\AppData\\Local\\Temp\\ipykernel_24192\\953103498.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning on the SFT data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning complete. Model saved to ./finetuned_model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# ----- Step 1: Load and Parse SFT Data -----\n",
    "# Read the SFT_data.txt file (which contains our fine-tuning example)\n",
    "sft_file = \"SFT_data.txt\"\n",
    "with open(sft_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    sft_text = f.read()\n",
    "\n",
    "# We assume the file contains a header for the input and then a section for the output.\n",
    "# In our case, the file uses \"#### Product Recommendations\" as the delimiter.\n",
    "parts = sft_text.split(\"#### Product Recommendations\")\n",
    "if len(parts) < 2:\n",
    "    raise ValueError(\"Could not split SFT_data.txt into input and output parts using the delimiter.\")\n",
    "# Everything before the delimiter is our input; everything after is the target output.\n",
    "input_part = parts[0].strip()\n",
    "output_part = parts[1].strip()\n",
    "\n",
    "# Create a single fine-tuning example as a dictionary.\n",
    "example = {\"input\": input_part, \"output\": output_part}\n",
    "print(\"Parsed Fine-Tuning Example:\")\n",
    "print(\"Input Part:\\n\", input_part)\n",
    "print(\"\\nOutput Part:\\n\", output_part)\n",
    "\n",
    "# ----- Step 2: Build a Dataset -----\n",
    "# Create a dataset (here with a single example; later you can expand to many examples)\n",
    "sft_dataset = Dataset.from_dict({\n",
    "    \"input\": [example[\"input\"]],\n",
    "    \"output\": [example[\"output\"]]\n",
    "})\n",
    "\n",
    "# ----- Step 3: Prepare the Model and Tokenizer for Fine-Tuning -----\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# ----- Step 4: Tokenization Function -----\n",
    "# We combine the input and output using a clear delimiter for training.\n",
    "def tokenize_record(record):\n",
    "    # We use \"\\n\\n### Response:\\n\" as a delimiter between input and expected output.\n",
    "    full_text = record[\"input\"] + \"\\n\\n### Response:\\n\" + record[\"output\"]\n",
    "    return tokenizer(full_text, truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_dataset = sft_dataset.map(tokenize_record, batched=False, remove_columns=[\"input\", \"output\"])\n",
    "\n",
    "# ----- Step 5: Set Up Data Collator -----\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# ----- Step 6: Define Training Arguments -----\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                   # Adjust as needed\n",
    "    per_device_train_batch_size=1,        # For a single example fine-tuning, batch size 1 is sufficient\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=True,                            # Enable FP16 if your GPU supports it\n",
    ")\n",
    "\n",
    "# ----- Step 7: Initialize the Trainer and Fine-Tune -----\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting fine-tuning on the SFT data...\")\n",
    "trainer.train()\n",
    "\n",
    "# ----- Step 8: Save the Fine-Tuned Model -----\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "print(\"\\nFine-tuning complete. Model saved to ./finetuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Output:\n",
      " 1. Cat Tee: A classic white tee for everyday wear.\n",
      "2. Cat Tee: A beautiful white tee for those who prefer a simple design.\n",
      "3. Cat Tee: A stylish white tee for business or leisure.\n",
      "4. Cat Tee: A high-quality white tee for work or play.\n",
      "5. Cat Tee: A classic white tee for everyday comfort.\n",
      "6. Cat Tee: A beautiful white tee for those who prefer a simple design.\n",
      "7. Cat Tee: A high-quality white tee for business or leisure.\n",
      "8. Cat Tee: A stylish white tee for business or everyday occasions.\n",
      "9. Cat Tee: A high-quality white tee for business or leisure.\n",
      "10. Cat Tee: A beautiful white tee for those who prefer a simple\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer from the saved directory\n",
    "model_dir = \"./finetuned_model\"\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(model_dir, trust_remote_code=True)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "# Set device for inference (0 for CUDA if available, else -1 for CPU)\n",
    "device_ft = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Create a text-generation pipeline for the fine-tuned model\n",
    "ft_pipe = pipeline(\"text-generation\", model=model_ft, tokenizer=tokenizer_ft, device=device_ft)\n",
    "\n",
    "# Construct a sample prompt in the same format as used for SFT dataset creation\n",
    "sample_prompt = (\n",
    "    \"Customer Profile:\\nAge: 35, Membership: ACTIVE\\n\\n\"\n",
    "    \"Purchase History:\\n\"\n",
    "    \"- Purchased a white vest top made of soft organic cotton with adjustable straps.\\n\"\n",
    "    \"- Bought several Cat Tee's in white, grey, and pink for everyday comfort.\\n\"\n",
    "    \"- Also acquired a green crew sweater with a relaxed, cozy fit and a grey hoodie for a sporty look.\\n\\n\"\n",
    "    \"Based on the above purchase history and profile, please recommend 10 products that this customer is likely to enjoy next.\\n\\n\"\n",
    "    \"### Response:\"\n",
    ")\n",
    "\n",
    "# Generate recommendations using the fine-tuned model\n",
    "ft_output = ft_pipe(sample_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "\n",
    "# Extract and print the output after the delimiter\n",
    "generated_text = ft_output[0][\"generated_text\"]\n",
    "if \"### Response:\" in generated_text:\n",
    "    final_output = generated_text.split(\"### Response:\")[-1].strip()\n",
    "else:\n",
    "    final_output = generated_text.strip()\n",
    "\n",
    "print(\"Fine-Tuned Model Output:\\n\", final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
