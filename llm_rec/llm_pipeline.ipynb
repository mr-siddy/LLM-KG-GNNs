{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "articles = pd.read_csv('articles.csv')\n",
    "customers = pd.read_csv('customers.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "transactions_train = pd.read_csv('transactions_train.csv')\n",
    "\n",
    "# Display the first few rows and all columns of each dataset\n",
    "print(\"Articles Dataset:\")\n",
    "print(articles.head())\n",
    "print(articles.columns)\n",
    "print(\"\\nCustomers Dataset:\")\n",
    "print(customers.head())\n",
    "print(customers.columns)\n",
    "print(\"\\nSample Submission Dataset:\")\n",
    "print(sample_submission.head())\n",
    "print(sample_submission.columns)\n",
    "print(\"\\nTransactions Train Dataset:\")\n",
    "print(transactions_train.head())\n",
    "print(transactions_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def engineer_customer_features(customer_id, transactions_df, articles_df, customers_df, reference_date=None):\n",
    "    \"\"\"\n",
    "    Extract rich features for a customer based on their transaction history\n",
    "    \n",
    "    Args:\n",
    "        customer_id (str): The customer's unique identifier\n",
    "        transactions_df (pd.DataFrame): Transaction history data\n",
    "        articles_df (pd.DataFrame): Article metadata\n",
    "        customers_df (pd.DataFrame): Customer metadata\n",
    "        reference_date (datetime, optional): Reference date for time calculations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Structured feature dictionary\n",
    "    \"\"\"\n",
    "    # Set reference date if not provided\n",
    "    if reference_date is None:\n",
    "        reference_date = pd.to_datetime('2020-09-22')  # End of dataset\n",
    "    elif isinstance(reference_date, str):\n",
    "        reference_date = pd.to_datetime(reference_date)\n",
    "    \n",
    "    # Get customer metadata\n",
    "    customer_meta = customers_df[customers_df['customer_id'] == customer_id]\n",
    "    if customer_meta.empty:\n",
    "        customer_meta = {\n",
    "            'age': None,\n",
    "            'club_member_status': None,\n",
    "            'fashion_news_frequency': None\n",
    "        }\n",
    "    else:\n",
    "        customer_meta = customer_meta.iloc[0].to_dict()\n",
    "    \n",
    "    # Filter transactions for this customer\n",
    "    customer_txns = transactions_df[transactions_df['customer_id'] == customer_id].copy()\n",
    "    \n",
    "    # Handle empty transaction history\n",
    "    if customer_txns.empty:\n",
    "        return {\n",
    "            'customer_id': customer_id,\n",
    "            'customer_meta': customer_meta,\n",
    "            'purchase_history': [],\n",
    "            'product_preferences': {},\n",
    "            'temporal_patterns': {},\n",
    "            'style_preferences': {}\n",
    "        }\n",
    "    \n",
    "    # Ensure datetime format\n",
    "    customer_txns['t_dat'] = pd.to_datetime(customer_txns['t_dat'])\n",
    "    \n",
    "    # Sort by date\n",
    "    customer_txns = customer_txns.sort_values('t_dat')\n",
    "    \n",
    "    # Enrich transactions with article data\n",
    "    enriched_txns = pd.merge(\n",
    "        customer_txns,\n",
    "        articles_df[['article_id', 'product_type_name', 'product_group_name', \n",
    "                     'colour_group_name', 'garment_group_name', 'index_group_name']],\n",
    "        on='article_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate purchase history features\n",
    "    purchase_history = {\n",
    "        'total_purchases': len(customer_txns),\n",
    "        'first_purchase_date': customer_txns['t_dat'].min().strftime('%Y-%m-%d'),\n",
    "        'last_purchase_date': customer_txns['t_dat'].max().strftime('%Y-%m-%d'),\n",
    "        'avg_price': float(customer_txns['price'].mean()),\n",
    "        'recent_article_ids': customer_txns.tail(10)['article_id'].tolist()\n",
    "    }\n",
    "    \n",
    "    # Calculate product preference features\n",
    "    product_preferences = {\n",
    "        'product_type': Counter(enriched_txns['product_type_name'].fillna('Unknown')).most_common(5),\n",
    "        'product_group': Counter(enriched_txns['product_group_name'].fillna('Unknown')).most_common(3),\n",
    "        'index_group': Counter(enriched_txns['index_group_name'].fillna('Unknown')).most_common(3)\n",
    "    }\n",
    "    \n",
    "    # Calculate temporal pattern features\n",
    "    enriched_txns['day_of_week'] = enriched_txns['t_dat'].dt.dayofweek\n",
    "    enriched_txns['month'] = enriched_txns['t_dat'].dt.month\n",
    "    \n",
    "    # Purchase day patterns\n",
    "    day_counts = Counter(enriched_txns['day_of_week'])\n",
    "    total_days = sum(day_counts.values())\n",
    "    day_distribution = {day: count/total_days for day, count in day_counts.items()}\n",
    "    \n",
    "    # Purchase month patterns\n",
    "    month_counts = Counter(enriched_txns['month'])\n",
    "    total_months = sum(month_counts.values())\n",
    "    month_distribution = {month: count/total_months for month, count in month_counts.items()}\n",
    "    \n",
    "    # Purchase frequency\n",
    "    if len(customer_txns) > 1:\n",
    "        date_diffs = []\n",
    "        dates = sorted(customer_txns['t_dat'].unique())\n",
    "        for i in range(1, len(dates)):\n",
    "            diff_days = (dates[i] - dates[i-1]).days\n",
    "            date_diffs.append(diff_days)\n",
    "        avg_days_between_purchases = sum(date_diffs) / len(date_diffs) if date_diffs else 0\n",
    "    else:\n",
    "        avg_days_between_purchases = 0\n",
    "    \n",
    "    # Use reference_date for days_since_last_purchase calculation\n",
    "    temporal_patterns = {\n",
    "        'day_distribution': day_distribution,\n",
    "        'month_distribution': month_distribution,\n",
    "        'avg_days_between_purchases': avg_days_between_purchases,\n",
    "        'days_since_last_purchase': (reference_date - customer_txns['t_dat'].max()).days\n",
    "    }\n",
    "    \n",
    "    # Calculate style preference features\n",
    "    style_preferences = {\n",
    "        'color': Counter(enriched_txns['colour_group_name'].fillna('Unknown')).most_common(5),\n",
    "        'garment_group': Counter(enriched_txns['garment_group_name'].fillna('Unknown')).most_common(3)\n",
    "    }\n",
    "    \n",
    "    # Assemble all features\n",
    "    features = {\n",
    "        'customer_id': customer_id,\n",
    "        'customer_meta': customer_meta,\n",
    "        'purchase_history': purchase_history,\n",
    "        'product_preferences': product_preferences,\n",
    "        'temporal_patterns': temporal_patterns,\n",
    "        'style_preferences': style_preferences\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(transactions_df, articles_df, customers_df, max_samples=10000, train_cutoff='2020-08-15'):\n",
    "    \"\"\"\n",
    "    Create a training dataset for the recommendation model\n",
    "    \n",
    "    Args:\n",
    "        transactions_df (pd.DataFrame): Transaction history data\n",
    "        articles_df (pd.DataFrame): Article metadata\n",
    "        customers_df (pd.DataFrame): Customer metadata\n",
    "        max_samples (int): Maximum number of customer samples to use\n",
    "        train_cutoff (str): Date to split training and target purchases\n",
    "        \n",
    "    Returns:\n",
    "        list: List of training examples with inputs and target outputs\n",
    "    \"\"\"\n",
    "    # Convert cutoff to datetime\n",
    "    cutoff_date = pd.to_datetime(train_cutoff)\n",
    "    \n",
    "    # Split transactions into training and target periods\n",
    "    transactions_df['t_dat'] = pd.to_datetime(transactions_df['t_dat'])\n",
    "    train_txns = transactions_df[transactions_df['t_dat'] < cutoff_date]\n",
    "    \n",
    "    # Target transactions: 7 days after cutoff\n",
    "    target_end_date = cutoff_date + pd.Timedelta(days=7)\n",
    "    target_txns = transactions_df[(transactions_df['t_dat'] >= cutoff_date) & \n",
    "                                 (transactions_df['t_dat'] < target_end_date)]\n",
    "    \n",
    "    # Find customers who have both training and target transactions\n",
    "    train_customers = set(train_txns['customer_id'].unique())\n",
    "    target_customers = set(target_txns['customer_id'].unique())\n",
    "    eligible_customers = list(train_customers.intersection(target_customers))\n",
    "    \n",
    "    # If we have too many customers, sample a subset\n",
    "    if len(eligible_customers) > max_samples:\n",
    "        import random\n",
    "        random.seed(42)  # For reproducibility\n",
    "        eligible_customers = random.sample(eligible_customers, max_samples)\n",
    "    \n",
    "    print(f\"Creating training data for {len(eligible_customers)} customers...\")\n",
    "    \n",
    "    # Create training examples\n",
    "    training_examples = []\n",
    "    \n",
    "    for i, customer_id in enumerate(eligible_customers):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing customer {i}/{len(eligible_customers)}\")\n",
    "        \n",
    "        # Get features from training period - pass the cutoff date as reference\n",
    "        features = engineer_customer_features(\n",
    "            customer_id, \n",
    "            train_txns, \n",
    "            articles_df, \n",
    "            customers_df,\n",
    "            reference_date=cutoff_date  # Use training cutoff as reference date\n",
    "        )\n",
    "        \n",
    "        # Get target articles from target period\n",
    "        target_articles = target_txns[target_txns['customer_id'] == customer_id]['article_id'].unique().tolist()\n",
    "        \n",
    "        # Create input prompt\n",
    "        input_prompt = format_prompt_from_features(features)\n",
    "        \n",
    "        # Create target output (customer_id, article_id1, article_id2, ...)\n",
    "        target_output = f\"{customer_id}, {', '.join(map(str, target_articles))}\"\n",
    "        \n",
    "        training_examples.append({\n",
    "            \"input\": input_prompt,\n",
    "            \"output\": target_output\n",
    "        })\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "def format_prompt_from_features(features):\n",
    "    \"\"\"\n",
    "    Format customer features into a text prompt for the model\n",
    "    \n",
    "    Args:\n",
    "        features (dict): Customer feature dictionary\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted prompt\n",
    "    \"\"\"\n",
    "    # Extract metadata\n",
    "    age = features['customer_meta'].get('age', 'Unknown')\n",
    "    membership = features['customer_meta'].get('club_member_status', 'Unknown')\n",
    "    fashion_news = features['customer_meta'].get('fashion_news_frequency', 'Unknown')\n",
    "    \n",
    "    # Purchase history\n",
    "    purchase_history = features['purchase_history']\n",
    "    total_purchases = purchase_history['total_purchases']\n",
    "    last_purchase = purchase_history['last_purchase_date']\n",
    "    avg_price = purchase_history['avg_price']\n",
    "    recent_articles = purchase_history['recent_article_ids'][-5:]  # Last 5 purchases\n",
    "    \n",
    "    # Product preferences\n",
    "    product_types = features['product_preferences']['product_type']\n",
    "    product_groups = features['product_preferences']['product_group']\n",
    "    \n",
    "    # Style preferences\n",
    "    colors = features['style_preferences']['color']\n",
    "    garment_groups = features['style_preferences']['garment_group']\n",
    "    \n",
    "    # Temporal patterns\n",
    "    days_since_last = features['temporal_patterns']['days_since_last_purchase']\n",
    "    purchase_frequency = 30 / max(1, features['temporal_patterns']['avg_days_between_purchases']) if features['temporal_patterns']['avg_days_between_purchases'] > 0 else 0\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt = f\"\"\"Customer {features['customer_id']} details:\n",
    "- Age: {age}\n",
    "- Membership Status: {membership}\n",
    "- Fashion News Frequency: {fashion_news}\n",
    "\n",
    "Purchase History:\n",
    "- Total Purchases: {total_purchases}\n",
    "- Last Purchase Date: {last_purchase} ({days_since_last} days ago)\n",
    "- Purchase Frequency: {purchase_frequency:.2f} items per month\n",
    "- Average Price Point: {avg_price:.4f}\n",
    "- Recent Purchases: {', '.join(map(str, recent_articles))}\n",
    "\n",
    "Product Preferences:\n",
    "- Favorite Product Types: {', '.join([f\"{p[0]} ({p[1]} purchases)\" for p in product_types[:3]])}\n",
    "- Favorite Product Groups: {', '.join([f\"{p[0]} ({p[1]} purchases)\" for p in product_groups[:2]])}\n",
    "- Favorite Garment Groups: {', '.join([f\"{g[0]} ({g[1]} purchases)\" for g in garment_groups[:2]])}\n",
    "\n",
    "Style Preferences:\n",
    "- Color Preferences: {', '.join([f\"{c[0]} ({c[1]} purchases)\" for c in colors[:3]])}\n",
    "\n",
    "Based on this customer's profile and purchase history, recommend the specific article IDs they are most likely to purchase in the next 7 days.\n",
    "\n",
    "Recommendations:\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_training_data(transactions_df, articles_df, customers_df, output_path='train_data.csv', samples=5000):\n",
    "    \"\"\"\n",
    "    Build and save the training dataset\n",
    "    \n",
    "    Args:\n",
    "        transactions_df (pd.DataFrame): Transaction history data\n",
    "        articles_df (pd.DataFrame): Article metadata\n",
    "        customers_df (pd.DataFrame): Customer metadata\n",
    "        output_path (str): Path to save the dataset\n",
    "        samples (int): Number of examples to generate\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The generated training data\n",
    "    \"\"\"\n",
    "    # Create training examples\n",
    "    training_examples = create_training_dataset(\n",
    "        transactions_df, \n",
    "        articles_df, \n",
    "        customers_df, \n",
    "        max_samples=samples\n",
    "    )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    train_df = pd.DataFrame(training_examples)\n",
    "    \n",
    "    # Save to CSV\n",
    "    train_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Training data saved to {output_path}\")\n",
    "    print(f\"Total examples: {len(train_df)}\")\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_data(train_df, transactions_df, articles_df):\n",
    "    \"\"\"\n",
    "    Analyze the quality of the generated training data\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): The training data\n",
    "        transactions_df (pd.DataFrame): Original transaction data\n",
    "        articles_df (pd.DataFrame): Article metadata\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis statistics\n",
    "    \"\"\"\n",
    "    print(f\"Total training examples: {len(train_df)}\")\n",
    "    \n",
    "    # Check input prompt length\n",
    "    prompt_lengths = train_df['input'].str.len()\n",
    "    print(f\"Prompt length statistics:\")\n",
    "    print(f\"  Mean: {prompt_lengths.mean():.2f} characters\")\n",
    "    print(f\"  Min: {prompt_lengths.min()} characters\")\n",
    "    print(f\"  Max: {prompt_lengths.max()} characters\")\n",
    "    \n",
    "    # Check target output\n",
    "    # Extract article counts per example\n",
    "    article_counts = []\n",
    "    for output in train_df['output']:\n",
    "        parts = output.split(',')\n",
    "        if len(parts) > 1:  # First part is customer_id\n",
    "            article_counts.append(len(parts) - 1)\n",
    "        else:\n",
    "            article_counts.append(0)\n",
    "    \n",
    "    print(f\"Target article count statistics:\")\n",
    "    print(f\"  Mean: {np.mean(article_counts):.2f} articles per customer\")\n",
    "    print(f\"  Min: {min(article_counts)} articles\")\n",
    "    print(f\"  Max: {max(article_counts)} articles\")\n",
    "    \n",
    "    # Check distribution of article types in targets\n",
    "    all_target_articles = []\n",
    "    for output in train_df['output']:\n",
    "        parts = output.split(',')\n",
    "        if len(parts) > 1:\n",
    "            articles = [int(a.strip()) for a in parts[1:] if a.strip().isdigit()]\n",
    "            all_target_articles.extend(articles)\n",
    "    \n",
    "    # Get article metadata for targets\n",
    "    target_articles_df = articles_df[articles_df['article_id'].isin(all_target_articles)]\n",
    "    \n",
    "    # Product type distribution\n",
    "    product_type_dist = target_articles_df['product_type_name'].value_counts().head(10)\n",
    "    print(\"\\nTop 10 product types in target recommendations:\")\n",
    "    for ptype, count in product_type_dist.items():\n",
    "        print(f\"  {ptype}: {count} ({count/len(all_target_articles)*100:.2f}%)\")\n",
    "    \n",
    "    # Return analysis results\n",
    "    return {\n",
    "        'example_count': len(train_df),\n",
    "        'prompt_length': {\n",
    "            'mean': prompt_lengths.mean(),\n",
    "            'min': prompt_lengths.min(),\n",
    "            'max': prompt_lengths.max()\n",
    "        },\n",
    "        'article_counts': {\n",
    "            'mean': np.mean(article_counts),\n",
    "            'min': min(article_counts),\n",
    "            'max': max(article_counts)\n",
    "        },\n",
    "        'top_product_types': product_type_dist.to_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the training data creation pipeline\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    articles_df = pd.read_csv(\"articles.csv\")\n",
    "    customers_df = pd.read_csv(\"customers.csv\")\n",
    "    transactions_df = pd.read_csv(\"transactions_train.csv\")\n",
    "    \n",
    "    print(\"Preparing training dataset...\")\n",
    "    # Use a smaller sample for initial testing\n",
    "    train_df = build_and_save_training_data(\n",
    "        transactions_df,\n",
    "        articles_df,\n",
    "        customers_df,\n",
    "        samples=1000  # Start with smaller sample for testing\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAnalyzing training data quality...\")\n",
    "    analysis = analyze_training_data(train_df, transactions_df, articles_df)\n",
    "    \n",
    "    # Create train/validation split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Save splits\n",
    "    train_data.to_csv(\"train_data_train.csv\", index=False)\n",
    "    val_data.to_csv(\"train_data_val.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nSaved {len(train_data)} training examples and {len(val_data)} validation examples\")\n",
    "    \n",
    "    # Show example\n",
    "    print(\"\\nExample training input:\")\n",
    "    print(train_df['input'].iloc[0][:500] + \"...\")\n",
    "    print(\"\\nExample training output:\")\n",
    "    print(train_df['output'].iloc[0])\n",
    "    \n",
    "    return train_df, analysis\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    train_df, analysis = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/opt-1.3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset columns: ['input', 'output']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_config, get_peft_model, TaskType\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load the Training Dataset\n",
    "# ------------------------------\n",
    "data_files = {\"train\": \"train_data_train.csv\", \"validation\": \"train_data_val.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "print(\"Training dataset columns:\", dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU name: NVIDIA GeForce RTX 4090\n",
      "GPU memory: 25.756696576 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bunny\\.cache\\huggingface\\hub\\models--qwen--qwen-7b-chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B-Chat:\n",
      "- cpp_kernels.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B-Chat:\n",
      "- qwen_generation_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B-Chat:\n",
      "- cpp_kernels.py\n",
      "- qwen_generation_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards:  12%|█▎        | 1/8 [05:01<35:07, 301.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen-7B-Chat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Model repository ID on Hugging Face\u001b[39;00m\n\u001b[0;32m     20\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    561\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\transformers\\modeling_utils.py:4022\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4019\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   4020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m   4021\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 4022\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4031\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4034\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4035\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4038\u001b[0m     is_safetensors_available()\n\u001b[0;32m   4039\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   4040\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4041\u001b[0m ):\n\u001b[0;32m   4042\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\transformers\\utils\\hub.py:1037\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[1;32m-> 1037\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\transformers\\utils\\hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    357\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    860\u001b[0m     )\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\huggingface_hub\\file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\huggingface_hub\\file_download.py:1547\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1544\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1545\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1547\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1556\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1557\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\huggingface_hub\\file_download.py:454\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    452\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    456\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\urllib3\\response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\urllib3\\response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\bunny\\anaconda3\\envs\\llm_gnn\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_config, get_peft_model, TaskType\n",
    "\n",
    "# Set environment variables for debugging and GPU configuration\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load Model and Tokenizer (Qwen 7B Chat)\n",
    "# ------------------------------\n",
    "model_name = \"Qwen/Qwen-7B-Chat\"  # Model repository ID on Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load the Training Dataset\n",
    "# ------------------------------\n",
    "data_files = {\"train\": \"train_data_train.csv\", \"validation\": \"train_data_val.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "print(\"Training dataset columns:\", dataset[\"train\"].column_names)\n",
    "\n",
    "# Check for empty inputs/outputs\n",
    "print(\"Checking for empty entries...\")\n",
    "empty_inputs = [i for i, x in enumerate(dataset[\"train\"]) if not x[\"input\"] or len(x[\"input\"]) == 0]\n",
    "empty_outputs = [i for i, x in enumerate(dataset[\"train\"]) if not x[\"output\"] or len(x[\"output\"]) == 0]\n",
    "print(f\"Empty inputs: {empty_inputs}\")\n",
    "print(f\"Empty outputs: {empty_outputs}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Set Up LoRA Fine-Tuning (PEFT)\n",
    "# ------------------------------\n",
    "peft_config = get_peft_config({\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"task_type\": TaskType.CAUSAL_LM,\n",
    "    \"inference_mode\": False,\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Target modules for OPT/Qwen style models\n",
    "})\n",
    "model = get_peft_model(model, peft_config, autocast_adapter_dtype=torch.float32)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Preprocessing Function\n",
    "# ------------------------------\n",
    "def preprocess_function(examples):\n",
    "    combined_texts = []\n",
    "    prompt_lengths = []\n",
    "    \n",
    "    # For each example, concatenate prompt and target\n",
    "    for inp, target in zip(examples[\"input\"], examples[\"output\"]):\n",
    "        prompt = f\"Input: {inp}\\nOutput:\"\n",
    "        combined = prompt + \" \" + target\n",
    "        combined_texts.append(combined)\n",
    "        \n",
    "        # Compute prompt token length (with special tokens)\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)[\"input_ids\"]\n",
    "        prompt_lengths.append(len(prompt_tokens))\n",
    "    \n",
    "    # Tokenize combined text without padding (we will let the collator pad)\n",
    "    tokenized = tokenizer(\n",
    "        combined_texts, \n",
    "        max_length=384,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Create labels with prompt tokens masked (-100)\n",
    "    labels = []\n",
    "    for i, ids in enumerate(tokenized[\"input_ids\"]):\n",
    "        label_ids = ids.copy()\n",
    "        prompt_len = min(prompt_lengths[i], len(label_ids))\n",
    "        label_ids[:prompt_len] = [-100] * prompt_len\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# Process datasets\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Processing training dataset\"\n",
    ")\n",
    "tokenized_val = dataset[\"validation\"].map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    "    desc=\"Processing validation dataset\"\n",
    ")\n",
    "\n",
    "# Create a proper data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Training Arguments and Trainer Setup\n",
    "# ------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen7b_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.05,\n",
    "    max_grad_norm=0.5,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Fine-Tuning\n",
    "# ------------------------------\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(\"./qwen7b_finetuned_final\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Input: Customer 24fe302f04d039c18035f650f3702aab8ef038fa3a8be71a03168e67eb176eb0 details:\n",
      "- Age: 48.0\n",
      "- Mem...\n",
      "Output: 24fe302f04d039c18035f650f3702aab8ef038fa3a8be71a03168e67eb176eb0, 762096003, 762096005\n",
      "\n",
      "Example 1:\n",
      "Input: Customer 361294c454f95944fafc18dc22e45a3093a41c636a77ad92b12bec7de43249f9 details:\n",
      "- Age: 24.0\n",
      "- Mem...\n",
      "Output: 361294c454f95944fafc18dc22e45a3093a41c636a77ad92b12bec7de43249f9, 754238023\n",
      "\n",
      "Example 2:\n",
      "Input: Customer 2ae89a0dd849301a82bbc69a646e4df18c31f60bad72e8ef2e9caffe611f46cb details:\n",
      "- Age: 25.0\n",
      "- Mem...\n",
      "Output: 2ae89a0dd849301a82bbc69a646e4df18c31f60bad72e8ef2e9caffe611f46cb, 713699005\n",
      "\n",
      "Example 3:\n",
      "Input: Customer 7e0a41e39052a62e30cbe0d370518516a3919e307ea6fc8cfad7b24bdd6488cb details:\n",
      "- Age: 55.0\n",
      "- Mem...\n",
      "Output: 7e0a41e39052a62e30cbe0d370518516a3919e307ea6fc8cfad7b24bdd6488cb, 757303021, 859125005, 713997031, 850251002\n",
      "\n",
      "Example 4:\n",
      "Input: Customer 6c051182fe486826cdd331070e5af84f481293d7e61af6c86397dfa04284b534 details:\n",
      "- Age: 25.0\n",
      "- Mem...\n",
      "Output: 6c051182fe486826cdd331070e5af84f481293d7e61af6c86397dfa04284b534, 891322004, 910264001, 909601001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some examples to verify format\n",
    "for i in range(min(5, len(dataset['train']))):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"Input: {dataset['train'][i]['input'][:100]}...\")\n",
    "    print(f\"Output: {dataset['train'][i]['output']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
